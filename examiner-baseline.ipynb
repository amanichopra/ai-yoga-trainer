{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5178941,"sourceType":"datasetVersion","datasetId":3010826},{"sourceId":1559111,"sourceType":"datasetVersion","datasetId":920599}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n# #         print(os.path.join(dirname, filename))\n#         pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ResNetModel,ResNetConfig,MobileNetV2Config, MobileNetV2Model,MobileNetV1Config, MobileNetV1Model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poses = []\nposes_to_path_train = {}\nposes_to_path_test = {}\nposes_to_path_val = {}\nfor dirname, _, filenames in os.walk('/kaggle/input/yoga-82/train/'):\n    _dirname = dirname.split('/')[-1].lower()\n    poses.append(_dirname)\n    poses_to_path_train[_dirname] = []\n    for filename in filenames:\n        poses_to_path_train[_dirname].append(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk('/kaggle/input/yoga-82/test/'):\n    _dirname = dirname.split('/')[-1].lower()\n    poses_to_path_test[_dirname] = []\n    for filename in filenames:\n        poses_to_path_test[_dirname].append(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('/kaggle/input/yoga-82/valid/'):\n    _dirname = dirname.split('/')[-1].lower()\n    poses_to_path_val[_dirname] = []\n    for filename in filenames:\n        poses_to_path_val[_dirname].append(os.path.join(dirname, filename))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poses=[i.split('/')[-1].lower() for i in poses[1:]]\ndel poses_to_path_train['']\ndel poses_to_path_test['']\ndel poses_to_path_val['']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poses_to_idx = dict((j,i) for i,j in enumerate(poses))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nclass YogaPoseDataset(Dataset):\n    def __init__(self,poses_to_path,poses_to_idx,poses,transform=None):\n        self.poses_to_path = poses_to_path\n        self.max_size = max(len(i) for i in poses_to_path)\n        self.poses_to_idx = poses_to_idx\n        self.class_num = len(self.poses_to_idx)\n        self.poses=poses\n        self.transform=transform\n    def __len__(self):\n        return self.max_size*self.class_num\n    def __getitem__(self,idx):\n        pose_class = idx%self.class_num\n        img_id = (idx//self.class_num)%len(self.poses_to_path[self.poses[pose_class]])\n        img = Image.open(self.poses_to_path[self.poses[pose_class]][img_id]).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img,pose_class\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms as T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = T.Compose([\n        T.Resize((128,128), interpolation=T.InterpolationMode.BILINEAR),\n        T.ToTensor(),\n        T.Normalize([0.5], [0.5]),\n        T.RandomErasing(p=1.0,value=\"random\"),\n        T.ColorJitter(),\n        T.RandomRotation((-45,45)),\n        T.RandomHorizontalFlip(p=0.5)\n]\n)\n\nval_trans = T.Compose([\n        T.Resize((128,128), interpolation=T.InterpolationMode.BILINEAR),\n        T.ToTensor(),\n        T.Normalize([0.5], [0.5]),\n]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = YogaPoseDataset(poses_to_path_train,poses_to_idx,poses,transform)\nval_ds = YogaPoseDataset(poses_to_path_val,poses_to_idx,poses,val_trans)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_ds,val_ds = random_split(ds,[0.8,0.2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = ResNetModel(ResNetConfig(depths=[2,2,2,2]))\nmodel = MobileNetV2Model(MobileNetV2Config())\n# model = MobileNetV1Model(MobileNetV1Config())\nmlp = nn.Sequential(\n#         # mobilenetv2\n    nn.BatchNorm1d(1280),\n    nn.Dropout(0.7),\n    nn.ReLU(),\n    nn.Linear(1280,256),\n#     # mobilenetv2\n#     nn.BatchNorm1d(1024),\n#     nn.Dropout(0.5),\n#     nn.Linear(1024,256),\n#     nn.BatchNorm1d(model.config.hidden_sizes[-1]),\n#     nn.ReLU(),\n#     nn.Dropout(0.5),\n#     nn.Linear(model.config.hidden_sizes[-1],train_ds.class_num),\n# #     nn.Linear(model.config.hidden_sizes[-1],256),\n    nn.Dropout(0.7),\n    nn.BatchNorm1d(256),\n    nn.ReLU(),\n    nn.Linear(256,train_ds.class_num),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\ntrain_dl = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\nval_dl  = DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = torch.optim.AdamW([*model.parameters(),*mlp.parameters()],lr=5e-4,weight_decay=5e-4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\"\n\nmodel.to(device)\nmlp.to(device)\n\n\nfor epoch in range(500):\n    model.train()\n    mlp.train()\n    tr_loss = []\n    res = []\n    for step, batch in enumerate(train_dl):\n        imgs,labels = batch\n        encodings = model(imgs.to(device))\n        logits = mlp(encodings.pooler_output.flatten(1))\n        loss = nn.functional.cross_entropy(logits,labels.to(device))\n        tr_loss.append(loss.detach().cpu())\n        res.append(torch.argmax(logits,dim=1).detach()==labels.to(device))\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    tr_loss = torch.stack(tr_loss).float().mean() \n    tr_acc = torch.cat([i for i in res]).float().mean()\n    model.eval()\n    mlp.eval()\n    res = []\n    val_loss = []\n    for step, batch in enumerate(val_dl):\n        with torch.no_grad():\n            imgs,labels = batch\n            encodings = model(imgs.to(device))\n            logits = mlp(encodings.pooler_output.flatten(1))\n            loss = nn.functional.cross_entropy(logits,labels.to(device))\n            res.append(torch.argmax(logits,dim=1)==labels.to(device))\n            val_loss.append(loss.detach())\n    print(f\"{epoch} Tr loss: {tr_loss} Tr acc: {tr_acc}\",\"Val loss: \",torch.stack(val_loss).float().mean(),\"Val acc: \",torch.cat([i for i in res]).float().mean())\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}